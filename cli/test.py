#!/usr/bin/env python3
"""
Test the trained BPE tokenizer by loading it from saved files.
"""

from pathlib import Path

from tokenizers import Tokenizer


def main(tokenizer_path: str = "./train-500k/tokenizer.json"):
    """Load and test a saved tokenizer with proper security handling."""

    print(f"Loading tokenizer from {tokenizer_path}...")

    # Check if tokenizer exists
    if not Path(tokenizer_path).exists():
        print(f"Tokenizer not found at {tokenizer_path}")
        print("Please run train_bpe.py first to train the tokenizer.")
        return None

    # Load tokenizer
    tokenizer = Tokenizer.from_file(tokenizer_path)

    # IMPORTANT: Set this to True to treat special tokens in user input as regular text
    tokenizer.encode_special_tokens = True

    print(f"Loaded tokenizer with vocab size: {tokenizer.get_vocab_size()}")

    # Test cases covering various Unicode scenarios
    print("\nTesting tokenizer...")
    test_cases = [
        "Hello world! This is a test with Ã©mojis ğŸš€",
        "Unicode test: cafÃ©, naÃ¯ve, rÃ©sumÃ©",
        "Emojis: ğŸ‘‹ ğŸŒ ğŸ”¥ ğŸ’¯ ğŸ‰",
        "Mixed: Hello ä¸–ç•Œ! Bonjour le monde ğŸŒŸ",
        "Special chars: <|startoftext|> content <|endoftext|>",
        "Edge case: ğ•³ğ–Šğ–‘ğ–‘ğ–” (mathematical symbols)",
    ]

    for test_text in test_cases:
        # Use add_special_tokens=False for raw user text to prevent injection
        encoding = tokenizer.encode(test_text, add_special_tokens=False)
        decoded = tokenizer.decode(encoding.ids)
        match = "âœ“" if test_text == decoded else "âœ—"
        print(f"{match} {test_text} -> {len(encoding.tokens)} tokens")

    # Comprehensive multilingual tests with larger text samples
    print("\n=== COMPREHENSIVE MULTILINGUAL TESTS ===")

    multilingual_tests = [
        {
            "name": "Spanish",
            "text": "Â¡Hola mundo! Este es un test en espaÃ±ol. Estamos probando que el tokenizador procese correctamente los caracteres en espaÃ±ol, incluyendo acentos y la letra Ã±. EspaÃ±a y AmÃ©rica Latina tienen una rica diversidad cultural.",
        },
        {
            "name": "Hindi",
            "text": "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾! à¤¯à¤¹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤­à¤¾à¤·à¤¾ à¤•à¤¾ à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤¹à¥ˆà¥¤ à¤¹à¤® à¤¦à¥‡à¤– à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤Ÿà¥‹à¤•à¤¨à¤¾à¤‡à¤œà¤¼à¤° à¤¦à¥‡à¤µà¤¨à¤¾à¤—à¤°à¥€ à¤²à¤¿à¤ªà¤¿ à¤•à¥‹ à¤¸à¤¹à¥€ à¤¤à¤°à¥€à¤•à¥‡ à¤¸à¥‡ à¤¸à¤‚à¤¸à¤¾à¤§à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤¯à¤¾ à¤¨à¤¹à¥€à¤‚à¥¤ à¤­à¤¾à¤°à¤¤ à¤à¤• à¤µà¤¿à¤µà¤¿à¤§à¤¤à¤¾à¤ªà¥‚à¤°à¥à¤£ à¤¦à¥‡à¤¶ à¤¹à¥ˆà¥¤",
        },
        {
            "name": "Russian",
            "text": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°? Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœÑ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹. Ğ Ğ¾ÑÑĞ¸Ñ â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ° Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞ¾ÑĞºĞ²Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ‚Ğ¾Ğ»Ğ¸Ñ†ĞµĞ¹ Ğ Ğ¾ÑÑĞ¸Ğ¸.",
        },
        {
            "name": "Chinese (Simplified)",
            "text": "ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡æµ‹è¯•ã€‚æˆ‘ä»¬æ­£åœ¨æµ‹è¯•åˆ†è¯å™¨æ˜¯å¦èƒ½æ­£ç¡®å¤„ç†ä¸­æ–‡å­—ç¬¦ã€‚ä¸­å›½æœ‰ç€æ‚ ä¹…çš„å†å²å’Œä¸°å¯Œçš„æ–‡åŒ–ã€‚åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œæœ‰å¾ˆå¤šè‘—åçš„æ™¯ç‚¹ã€‚",
        },
        {
            "name": "Japanese",
            "text": "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œï¼ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæ—¥æœ¬èªã®æ–‡å­—ã‚’æ­£ã—ãå‡¦ç†ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚æ—¥æœ¬ã¯ç¾ã—ã„å›½ã§ã€è±Šã‹ãªæ–‡åŒ–ã¨æ­´å²ãŒã‚ã‚Šã¾ã™ã€‚æ±äº¬ã¯æ—¥æœ¬ã®é¦–éƒ½ã§ã™ã€‚",
        },
        {
            "name": "German",
            "text": "Hallo Welt! Dies ist ein Test der deutschen Sprache. Wir testen, ob der Tokenizer deutsche Zeichen korrekt verarbeitet. Deutschland ist ein schÃ¶nes Land mit reicher Geschichte und Kultur. Berlin ist die Hauptstadt von Deutschland.",
        },
        {
            "name": "Mixed Languages",
            "text": "Hello world! Â¡Hola mundo! ä½ å¥½ä¸–ç•Œ! à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾! ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€! ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ! Hallo Welt! This sentence mixes multiple languages including Spanish, Chinese, Hindi, Russian, Japanese, and German.",
        },
    ]

    for test in multilingual_tests:
        print(f"\n--- {test['name']} Test ---")
        text = test["text"]
        encoding = tokenizer.encode(text, add_special_tokens=False)
        decoded = tokenizer.decode(encoding.ids)

        print(f"Original text ({len(text)} chars): {text}")
        print(f"Tokenized to {len(encoding.tokens)} tokens")
        print(f"First 10 tokens: {encoding.tokens[:10]}")
        print(f"Token IDs: {encoding.ids[:10]}...")
        print(f"Decoded text: {decoded}")
        print(f"Round-trip perfect: {'âœ“' if text == decoded else 'âœ—'}")

        if text != decoded:
            print("âš ï¸  MISMATCH DETECTED!")
            print(f"Original bytes: {text.encode('utf-8')[:50]}...")
            print(f"Decoded bytes:  {decoded.encode('utf-8')[:50]}...")

        # Calculate compression ratio
        compression_ratio = len(text.encode("utf-8")) / len(encoding.tokens)
        print(
            f"Compression: {len(text.encode('utf-8'))} bytes -> {len(encoding.tokens)} tokens (ratio: {compression_ratio:.2f})"
        )

    print("\n=== FILE-BASED MULTILINGUAL ANALYSIS ===")

    # Test files with substantial content in different languages
    test_files = [
        {"name": "English", "file": "sample_texts/english.txt"},
        {"name": "Spanish", "file": "sample_texts/spanish.txt"},
        {"name": "Chinese", "file": "sample_texts/chinese.txt"},
        {"name": "Russian", "file": "sample_texts/russian.txt"},
        {"name": "Japanese", "file": "sample_texts/japanese.txt"},
        {"name": "Hindi", "file": "sample_texts/hindi.txt"},
        {"name": "Arabic", "file": "sample_texts/arabic.txt"},
        {"name": "German", "file": "sample_texts/german.txt"},
    ]

    for test_file in test_files:
        file_path = Path(test_file["file"])
        if not file_path.exists():
            print(f"âš ï¸  {test_file['name']} file not found: {file_path}")
            continue

        print(f"\n--- {test_file['name']} File Analysis ---")

        # Read file content
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
        except Exception as e:
            print(f"âŒ Error reading {file_path}: {e}")
            continue

        # Tokenize content
        encoding = tokenizer.encode(content, add_special_tokens=False)
        decoded = tokenizer.decode(encoding.ids)

        # Calculate metrics
        char_count = len(content)
        byte_count = len(content.encode("utf-8"))
        token_count = len(encoding.tokens)
        word_count = len(content.split())

        # Compression ratios
        chars_per_token = char_count / token_count if token_count > 0 else 0
        bytes_per_token = byte_count / token_count if token_count > 0 else 0
        tokens_per_word = token_count / word_count if word_count > 0 else 0

        print(
            f"File size: {char_count:,} chars, {byte_count:,} bytes, {word_count:,} words"
        )
        print(f"Tokenized to: {token_count:,} tokens")
        print("Compression ratios:")
        print(f"  â€¢ {chars_per_token:.2f} chars/token")
        print(f"  â€¢ {bytes_per_token:.2f} bytes/token")
        print(f"  â€¢ {tokens_per_word:.2f} tokens/word")

        # Round-trip accuracy test
        accuracy = "âœ“ Perfect" if content == decoded else "âœ— MISMATCH"
        print(f"Round-trip accuracy: {accuracy}")

        if content != decoded:
            print("âš ï¸  Encoding/decoding mismatch detected!")
            # Show first difference
            for i, (orig, dec) in enumerate(zip(content, decoded)):
                if orig != dec:
                    print(f"   First difference at position {i}: '{orig}' vs '{dec}'")
                    break
            print(f"   Original length: {len(content)}, Decoded length: {len(decoded)}")

        # Show sample tokens for analysis
        sample_tokens = (
            encoding.tokens[:15] if len(encoding.tokens) >= 15 else encoding.tokens
        )
        print(f"Sample tokens: {sample_tokens}")

    print("\n=== TOKENIZER STATISTICS ===")
    print(f"Vocabulary size: {tokenizer.get_vocab_size():,}")
    print("File-based multilingual analysis completed!")

    # Test special token IDs
    start_id = tokenizer.token_to_id("<|startoftext|>")
    end_id = tokenizer.token_to_id("<|endoftext|>")
    print(f"\nSpecial tokens: <|startoftext|>={start_id}, <|endoftext|>={end_id}")

    # Show how to properly add special tokens programmatically
    print("\nProgrammatic special token usage:")
    content = "This is my content"
    content_tokens = tokenizer.encode(content, add_special_tokens=False)
    # Manually add special tokens by ID
    full_sequence = [start_id] + content_tokens.ids + [end_id]
    full_text = tokenizer.decode(full_sequence)
    print(f"Content: {content}")
    print(f"With special tokens: {full_text}")

    print("\nAll tests completed!")
    return tokenizer


if __name__ == "__main__":
    main()
