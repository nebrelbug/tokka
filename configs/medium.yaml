# Simple 50M configuration for multilingual tokenizer training
# Medium-scale setup with most common human languages and essential coding languages

training:
  total_samples: 50_000_000
  streaming_enabled: true
  output_dir: "./tokenizers/train-simple-50M"
  temperature: 0.3
  min_samples_per_lang: 500_000    # Higher minimum for better coverage
  max_samples_per_lang: 10_000_000 # Reasonable maximum for 50M total

datasets:
  # English - most common
  - path: "HuggingFaceFW/fineweb"
    description: "English web data"
    subsets:
      - name: "sample-10BT"
        priority: 5

  # Top 6 human languages as specified by user
  - path: "HuggingFaceFW/fineweb-2"
    description: "Multilingual web data"
    subsets:
      - name: "rus_Cyrl"  # Russian
        priority: 4
      - name: "cmn_Hani"  # Mandarin Chinese
        priority: 4
      - name: "deu_Latn"  # German
        priority: 4
      - name: "jpn_Jpan"  # Japanese
        priority: 4
      - name: "spa_Latn"  # Spanish
        priority: 4
      - name: "fra_Latn"  # French
        priority: 4

  # Essential programming languages - expanded
  - path: "bigcode/starcoderdata"
    description: "Programming languages"
    subsets:
      - name: "python"
        priority: 4
        text_column: "content"
      - name: "javascript"
        priority: 3
        text_column: "content"
      - name: "typescript"
        priority: 3
        text_column: "content"
      - name: "java"
        priority: 3
        text_column: "content" 