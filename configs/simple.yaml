# Simple 500K configuration for multilingual tokenizer training
# Basic setup with most common human languages and essential coding languages

training:
  total_samples: 500_000
  streaming_enabled: true
  output_dir: "./tokenizers/train-simple-500k"
  temperature: 0.3
  min_samples_per_lang: 10_000    # Lower minimum for small dataset
  max_samples_per_lang: 100_000   # Lower maximum for balanced distribution

datasets:
  # English - most common
  - path: "HuggingFaceFW/fineweb"
    description: "English web data"
    subsets:
      - name: "sample-10BT"
        priority: 5

  # # Top 5 human languages as specified by user
  # - path: "HuggingFaceFW/fineweb-2"
  #   description: "Multilingual web data"
  #   subsets:
  #     - name: "rus_Cyrl"  # Russian
  #       priority: 4
  #     - name: "cmn_Hani"  # Mandarin Chinese
  #       priority: 4
  #     - name: "deu_Latn"  # German
  #       priority: 4
  #     - name: "jpn_Jpan"  # Japanese
  #       priority: 4
  #     - name: "spa_Latn"  # Spanish
  #       priority: 4
  #     - name: "fra_Latn"  # French
  #       priority: 4

  # # Essential programming languages
  # - path: "bigcode/starcoderdata"
  #   description: "Programming languages"
  #   subsets:
  #     - name: "python"
  #       priority: 4
  #       text_column: "content"
  #     - name: "javascript"
  #       priority: 3
  #       text_column: "content"