# Simple 64K configuration for multilingual tokenizer training
# Basic setup with most essential languages and core programming languages

total_samples: 1_000_000
streaming_enabled: true
output_dir: "train-simple-1M"
temperature: 0.3
min_samples_per_lang: 50_000    # Minimum for good coverage
max_samples_per_lang: 300_000   # Balanced distribution across 7 subsets
vocab_size: 64_000              # Medium vocabulary size

datasets:
  # TIER 1: Most essential languages (highest priority)
  - path: "HuggingFaceFW/fineweb"
    description: "English web data"
    subsets:
      - name: "sample-10BT"
        priority: 5

  - path: "HuggingFaceFW/fineweb-2"
    description: "Core multilingual languages"
    subsets:
      - name: "cmn_Hani"  # Chinese (Mandarin)
        priority: 4
      - name: "spa_Latn"  # Spanish
        priority: 4
      - name: "fra_Latn"  # French
        priority: 4

  # TIER 2: Essential programming languages
  - path: "bigcode/starcoderdata"
    description: "Core programming languages"
    subsets:
      - name: "python"
        priority: 4
        text_column: "content"
      - name: "javascript"
        priority: 3
        text_column: "content"
      - name: "java"
        priority: 3
        text_column: "content"